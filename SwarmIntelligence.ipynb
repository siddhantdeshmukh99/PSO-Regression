{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SwarmIntelligence.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qf2F9BUh3Bm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import datasets\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import norm\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "data = datasets.load_boston()\n",
        "X = pd.DataFrame(data.data,columns=data.feature_names).values\n",
        "y= data.target\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nzcs0ojvpwN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "94201390-64c9-4eee-de2a-5bb2f3914f05"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(506, 13)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVw1icA6jw8W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SIregression:\n",
        "  def __init__(self,num_particles=10,learning_rate=0.0003,batch_size=10,epoch=10,inertia = 0.9, c1 = 0.5, c2 = 0.4):\n",
        "    self.lr_init = learning_rate\n",
        "    self.batch_size = batch_size\n",
        "    self.num_particles = num_particles\n",
        "    self.trained_weights = None\n",
        "    self.epoch = epoch\n",
        "    self.c2 = c2\n",
        "    self.c1 = c1\n",
        "    self.inertia = inertia\n",
        "\n",
        "  def split(self,data):\n",
        "    # one can override this function build custom data spliter (for temporal data)\n",
        "    split_percentage = 1\n",
        "    # returns training data,validation data\n",
        "    return data[:int(data.shape[0]*split_percentage)],data[int(data.shape[0]*(split_percentage)):] \n",
        "\n",
        "  def loss_func(self,target,values):\n",
        "    # one can overrided this function to create a custome lossfunction\n",
        "    return np.sum((target-values)**2)/target.shape[0]\n",
        "\n",
        "  def exp_decay(self,loss):\n",
        "    # learning rate will decay with respect to the loss\n",
        "    k = 0.3\n",
        "    lrate = self.lr_init * np.exp(k*(loss-self.thresh))\n",
        "    return lrate\n",
        "\n",
        "  def velocity_update(self,v1,pbest,gbest,present):\n",
        "    #updating the velocity using the formula\n",
        "    # logistic equation of chaos as r > 4 \n",
        "    self.inertia = 4*self.inertia*(1-self.inertia)\n",
        "    return self.inertia*v1 + self.c1*np.random.rand()*(pbest-present)+self.c2*np.random.rand()*(gbest-present) \n",
        "\n",
        "  def fit(self,X,y):\n",
        "    # initializing starting position, velocity, particles best values\n",
        "    particles=[]\n",
        "    best_loss = 0\n",
        "    global_best = None\n",
        "    # adding one for the bias term\n",
        "    X = np.append(X,np.ones(shape = (X.shape[0],1)),axis=1)\n",
        "\n",
        "    for particle in norm.rvs(scale = 5, size = (self.num_particles , X.shape[1])):\n",
        "      particles.append({'values':particle,\n",
        "                        'velocity':np.random.randn(X.shape[1]),\n",
        "                        'present_best': particle})\n",
        "      \n",
        "      if len(particles) == 1:\n",
        "        global_best = particle\n",
        "        best_loss = self.loss_func(y[:self.batch_size],np.array([np.sum(particle*x) for x in X[:self.batch_size]]))\n",
        "      else:\n",
        "        current_loss = self.loss_func(y[:self.batch_size],np.array([np.sum(particle*x) for x in X[:self.batch_size]]))\n",
        "        if current_loss<best_loss:\n",
        "          global_best = particle\n",
        "          best_loss = current_loss\n",
        "    particles = np.array(particles)\n",
        "\n",
        "    #algorithm\n",
        "    ep = 0\n",
        "    loss = 9999\n",
        "    while self.epoch>ep:\n",
        "      train_data,val_data = self.split(np.append(X,np.vstack(y),axis=1))\n",
        "      previous_loss = self.loss_func(train_data[:,-1],np.array([np.sum(global_best*x) for x in train_data[:,:-1]]))\n",
        "      for i,particle in enumerate(particles):\n",
        "        # print(particle.values)\n",
        "        particles[i]['velocity'] = self.velocity_update(particle['velocity'],global_best,particle['present_best'],particles[i]['values'])\n",
        "        \n",
        "        # updating the position\n",
        "        particles[i]['values'] = particles[i]['values'] + particles[i]['velocity']\n",
        "\n",
        "        # updating present best and global best\n",
        "        present_loss = self.loss_func(train_data[:,-1],np.array([np.sum(particles[i]['values']*x) for x in train_data[:,:-1]]))\n",
        "        present_best_loss = self.loss_func(train_data[:,-1],np.array([np.sum(particles[i]['present_best']*x) for x in train_data[:,:-1]]))\n",
        "        global_best_loss = self.loss_func(train_data[:,-1],np.array([np.sum(global_best*x) for x in train_data[:,:-1]]))\n",
        "        \n",
        "        if present_loss < present_best_loss:\n",
        "          particles[i]['present_best'] = particles[i]['values']\n",
        "        if present_best_loss < global_best_loss:\n",
        "          global_best = particles[i]['present_best']\n",
        "        \n",
        "      loss = self.loss_func(train_data[:,-1],np.array([np.sum(global_best*x) for x in train_data[:,:-1]]))\n",
        "      # mutation for local minima purpose\n",
        "      if int(previous_loss) == int(loss):\n",
        "        for i,_ in enumerate(particles):\n",
        "          # shift the point by a randnom distance which decreases as the iterations reaches end\n",
        "          particles[i]['values'] = particles[i]['values'] + norm.rvs(scale =(self.epoch-ep)/self.epoch if (self.epoch-ep)/self.epoch > 0.2 else 0.2 ,size=(len(particles[i]['values'])))\n",
        "      \n",
        "      print(\"training loss {} of epoch {}\".format(loss,ep+1))\n",
        "      ep = ep+1\n",
        "    \n",
        "    print(global_best.shape)\n",
        "    self.trained_weights = global_best\n",
        "    return self.trained_weights\n",
        "\n",
        "  def predict(self,X):\n",
        "    # adding one for the bias term\n",
        "    print(X.shape)\n",
        "    X = np.append(X,np.ones(shape = (X.shape[0],1)),axis=1)\n",
        "    return np.array([np.sum(self.trained_weights*x) for x in X[:,:]])"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5s-rc0Wc1lV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 907
        },
        "outputId": "b73742fd-b43e-47b7-8520-2b2f53dd8162"
      },
      "source": [
        "model = SIregression(num_particles=70,epoch=50)\n",
        "model.fit(X[:-50],y[:-50])\n",
        "print(model.loss_func(y[-50:],model.predict(X[-50:])))"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training loss 154.5629408829471 of epoch 1\n",
            "training loss 154.5629408829471 of epoch 2\n",
            "training loss 137.02750907496628 of epoch 3\n",
            "training loss 105.03039722149933 of epoch 4\n",
            "training loss 105.03039722149933 of epoch 5\n",
            "training loss 81.20740329737849 of epoch 6\n",
            "training loss 74.57354573706556 of epoch 7\n",
            "training loss 74.57354573706556 of epoch 8\n",
            "training loss 72.74709778176567 of epoch 9\n",
            "training loss 67.85406094575369 of epoch 10\n",
            "training loss 66.08700702495035 of epoch 11\n",
            "training loss 59.43497181341694 of epoch 12\n",
            "training loss 53.719489654740485 of epoch 13\n",
            "training loss 52.31239671420372 of epoch 14\n",
            "training loss 50.06149107410644 of epoch 15\n",
            "training loss 50.06149107410644 of epoch 16\n",
            "training loss 43.52470940839406 of epoch 17\n",
            "training loss 42.48568453954056 of epoch 18\n",
            "training loss 41.580862780287276 of epoch 19\n",
            "training loss 38.668752681556775 of epoch 20\n",
            "training loss 35.86356035302212 of epoch 21\n",
            "training loss 35.86356035302212 of epoch 22\n",
            "training loss 34.60426791518303 of epoch 23\n",
            "training loss 33.78426867116769 of epoch 24\n",
            "training loss 33.78426867116769 of epoch 25\n",
            "training loss 32.741906710411826 of epoch 26\n",
            "training loss 30.245826776737776 of epoch 27\n",
            "training loss 30.039081519743455 of epoch 28\n",
            "training loss 29.038633862612738 of epoch 29\n",
            "training loss 28.099803486326635 of epoch 30\n",
            "training loss 27.713949080510663 of epoch 31\n",
            "training loss 27.65636265127709 of epoch 32\n",
            "training loss 26.84291495815481 of epoch 33\n",
            "training loss 26.33515123487858 of epoch 34\n",
            "training loss 26.33515123487858 of epoch 35\n",
            "training loss 26.08369530740414 of epoch 36\n",
            "training loss 26.08369530740414 of epoch 37\n",
            "training loss 25.80544930552118 of epoch 38\n",
            "training loss 25.50408995042212 of epoch 39\n",
            "training loss 25.466650912695837 of epoch 40\n",
            "training loss 25.466650912695837 of epoch 41\n",
            "training loss 25.006418233523263 of epoch 42\n",
            "training loss 25.006418233523263 of epoch 43\n",
            "training loss 24.791366621590967 of epoch 44\n",
            "training loss 24.739581790347398 of epoch 45\n",
            "training loss 24.682014982831852 of epoch 46\n",
            "training loss 24.62700636608834 of epoch 47\n",
            "training loss 24.450045962295448 of epoch 48\n",
            "training loss 24.308176616690417 of epoch 49\n",
            "training loss 24.308176616690417 of epoch 50\n",
            "(14,)\n",
            "(50, 13)\n",
            "10.340180613627904\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7vMayy4kLER",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a758d132-acf7-4c3f-ca87-adde52d09dbe"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "mod = LinearRegression()\n",
        "mod.fit(X[:-50],y[:-50])\n",
        "print(model.loss_func(y[-50:],mod.predict(X[-50:])))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10.96041067942274\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaYxpP_-kPCK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "77c978c2-3839-4917-cb0c-3bc63a24345f"
      },
      "source": [
        "mod.coef_"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.90758523,  1.1448883 ,  0.22037032,  0.6376401 , -2.04067241,\n",
              "        2.67970135,  0.29749879, -3.02099961,  3.14455125, -2.60489475,\n",
              "       -1.97322935,  0.89921549, -3.96002849])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSAk9Xvu0SP1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "ba81876a-42f3-4c73-acfd-a98ebba416f8"
      },
      "source": [
        "model.trained_weights"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.71736973,  0.86814684,  0.07047775,  0.73234371, -1.47926267,\n",
              "        2.77617494,  0.1899308 , -3.34406331,  0.93882257, -1.12369738,\n",
              "       -2.30365275,  1.10506165, -3.1393519 , 22.79075347])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    }
  ]
}